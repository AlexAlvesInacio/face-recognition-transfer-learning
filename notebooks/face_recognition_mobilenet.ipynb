{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4fc320ac-97dd-438b-ade3-9161e68a3b8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ Python executando em: C:\\Users\\scien\\.conda\\envs\\faceproj\\python.exe\n",
      "â„¹ï¸ VersÃ£o do Python: 3.10.19\n",
      "\n",
      "=== CHECAGEM DE VERSÃ•ES ===\n",
      "âœ… opencv-python: 4.11.0\n",
      "âœ… mediapipe: 0.10.14\n",
      "âœ… tensorflow: 2.16.1\n",
      "âœ… numpy: 1.26.4\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "print(f\"ğŸ Python executando em: {sys.executable}\")\n",
    "print(f\"â„¹ï¸ VersÃ£o do Python: {sys.version.split()[0]}\\n\")\n",
    "\n",
    "libs = {\n",
    "    \"opencv-python\": \"cv2\",\n",
    "    \"mediapipe\": \"mediapipe\",\n",
    "    \"tensorflow\": \"tensorflow\",\n",
    "    \"numpy\": \"numpy\"\n",
    "}\n",
    "\n",
    "print(\"=== CHECAGEM DE VERSÃ•ES ===\")\n",
    "for nome_pip, nome_import in libs.items():\n",
    "    try:\n",
    "        mod = __import__(nome_import)\n",
    "        # Alguns mÃ³dulos usam __version__, outros podem variar, mas o padrÃ£o Ã© esse\n",
    "        version = getattr(mod, \"__version__\", \"VersÃ£o nÃ£o encontrada\")\n",
    "        print(f\"âœ… {nome_pip}: {version}\")\n",
    "    except ImportError:\n",
    "        print(f\"âŒ {nome_pip}: NÃƒO INSTALADO (ou nÃ£o visÃ­vel para este Kernel)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cbf27ac5-bbd0-4242-bdab-6db9b843cec1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“¸ Coletando dados para: FABIANA\n",
      "Tecle [S] para Salvar | [Q] para Sair\n",
      "âœ… Foto salva! Total: 1\n",
      "âœ… Foto salva! Total: 2\n",
      "âœ… Foto salva! Total: 3\n",
      "âœ… Foto salva! Total: 4\n",
      "âœ… Foto salva! Total: 5\n",
      "âœ… Foto salva! Total: 6\n",
      "âœ… Foto salva! Total: 7\n",
      "âœ… Foto salva! Total: 8\n",
      "âœ… Foto salva! Total: 9\n",
      "âœ… Foto salva! Total: 10\n",
      "âœ… Foto salva! Total: 11\n",
      "âœ… Foto salva! Total: 12\n",
      "âœ… Foto salva! Total: 13\n",
      "âœ… Foto salva! Total: 14\n",
      "âœ… Foto salva! Total: 15\n",
      "âœ… Foto salva! Total: 16\n",
      "âœ… Foto salva! Total: 17\n",
      "âœ… Foto salva! Total: 18\n",
      "âœ… Foto salva! Total: 19\n",
      "âœ… Foto salva! Total: 20\n",
      "âœ… Foto salva! Total: 21\n",
      "âœ… Foto salva! Total: 22\n",
      "âœ… Foto salva! Total: 23\n",
      "âœ… Foto salva! Total: 24\n",
      "âœ… Foto salva! Total: 25\n",
      "âœ… Foto salva! Total: 26\n",
      "âœ… Foto salva! Total: 27\n",
      "âœ… Foto salva! Total: 28\n",
      "âœ… Foto salva! Total: 29\n",
      "âœ… Foto salva! Total: 30\n",
      "âœ… Foto salva! Total: 31\n",
      "âœ… Foto salva! Total: 32\n",
      "âœ… Foto salva! Total: 33\n",
      "âœ… Foto salva! Total: 34\n",
      "âœ… Foto salva! Total: 35\n",
      "âœ… Foto salva! Total: 36\n",
      "âœ… Foto salva! Total: 37\n",
      "âœ… Foto salva! Total: 38\n",
      "âœ… Foto salva! Total: 39\n",
      "âœ… Foto salva! Total: 40\n",
      "âœ… Foto salva! Total: 41\n",
      "âœ… Foto salva! Total: 42\n",
      "âœ… Foto salva! Total: 43\n",
      "âœ… Foto salva! Total: 44\n",
      "âœ… Foto salva! Total: 45\n",
      "âœ… Foto salva! Total: 46\n",
      "âœ… Foto salva! Total: 47\n",
      "âœ… Foto salva! Total: 48\n",
      "âœ… Foto salva! Total: 49\n",
      "âœ… Foto salva! Total: 50\n",
      "âœ… Coleta finalizada.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import time\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "\n",
    "# ConfiguraÃ§Ãµes Globais\n",
    "RAW_DATA_DIR = os.path.join(\"data\", \"raw\")\n",
    "IMG_SIZE = 160 # Tamanho padrÃ£o para o modelo\n",
    "\n",
    "# Cria a pasta principal se nÃ£o existir\n",
    "os.makedirs(RAW_DATA_DIR, exist_ok=True)\n",
    "\n",
    "def coletar_dados(nome_pessoa, qtd_maxima=50):\n",
    "    save_path = os.path.join(RAW_DATA_DIR, nome_pessoa)\n",
    "    os.makedirs(save_path, exist_ok=True)\n",
    "    \n",
    "    cap = cv2.VideoCapture(0)\n",
    "    mp_face = mp.solutions.face_detection\n",
    "    \n",
    "    print(f\"ğŸ“¸ Coletando dados para: {nome_pessoa.upper()}\")\n",
    "    print(\"Tecle [S] para Salvar | [Q] para Sair\")\n",
    "    \n",
    "    count = len(os.listdir(save_path))\n",
    "    \n",
    "    with mp_face.FaceDetection(min_detection_confidence=0.6) as face:\n",
    "        while cap.isOpened():\n",
    "            ok, frame = cap.read()\n",
    "            if not ok: break\n",
    "            \n",
    "            frame = cv2.flip(frame, 1)\n",
    "            rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            results = face.process(rgb)\n",
    "            \n",
    "            face_crop = None\n",
    "            \n",
    "            if results.detections:\n",
    "                # Pega a maior detecÃ§Ã£o (caso tenha mais de uma pessoa)\n",
    "                detection = max(results.detections, key=lambda d: d.score[0])\n",
    "                bbox = detection.location_data.relative_bounding_box\n",
    "                \n",
    "                h, w, _ = frame.shape\n",
    "                x, y = int(bbox.xmin * w), int(bbox.ymin * h)\n",
    "                w_box, h_box = int(bbox.width * w), int(bbox.height * h)\n",
    "                \n",
    "                # Desenha o quadrado\n",
    "                cv2.rectangle(frame, (x, y), (x + w_box, y + h_box), (0,255,0), 2)\n",
    "                \n",
    "                # Recorte seguro (evita erro se sair da tela)\n",
    "                face_crop = frame[max(0, y):min(h, y+h_box), max(0, x):min(w, x+w_box)]\n",
    "            \n",
    "            cv2.putText(frame, f\"Salvas: {count}\", (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0,255,0), 2)\n",
    "            cv2.imshow(f\"Coleta - {nome_pessoa}\", frame)\n",
    "            \n",
    "            k = cv2.waitKey(1)\n",
    "            if k == ord('s') or k == ord('S'):\n",
    "                if face_crop is not None and face_crop.size > 0:\n",
    "                    face_resized = cv2.resize(face_crop, (IMG_SIZE, IMG_SIZE))\n",
    "                    file_name = f\"{nome_pessoa}_{int(time.time()*1000)}.jpg\"\n",
    "                    cv2.imwrite(os.path.join(save_path, file_name), face_resized)\n",
    "                    count += 1\n",
    "                    print(f\"âœ… Foto salva! Total: {count}\")\n",
    "                else:\n",
    "                    print(\"âš ï¸ Rosto nÃ£o detectado!\")\n",
    "            \n",
    "            if k == ord('q') or k == ord('Q') or count >= qtd_maxima:\n",
    "                break\n",
    "                \n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    print(\"âœ… Coleta finalizada.\")\n",
    "\n",
    "# --- EXECUÃ‡ÃƒO ---\n",
    "# Mude o nome aqui e rode a cÃ©lula (ex: \"alex\", depois mude para \"fabiana\" e rode de novo)\n",
    "coletar_dados(\"fabiana\", qtd_maxima=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "14738681-4ba0-4086-9a46-5d9de2c8efc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 100 files belonging to 2 classes.\n",
      "Using 80 files for training.\n",
      "Found 100 files belonging to 2 classes.\n",
      "Using 20 files for validation.\n",
      "\n",
      "ğŸ¤– Classes encontradas: ['alex', 'fabiana']\n",
      "Epoch 1/15\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 643ms/step - accuracy: 0.5500 - loss: 0.9319 - val_accuracy: 0.7000 - val_loss: 0.5238\n",
      "Epoch 2/15\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 157ms/step - accuracy: 0.7125 - loss: 0.6422 - val_accuracy: 0.9000 - val_loss: 0.3748\n",
      "Epoch 3/15\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 164ms/step - accuracy: 0.6625 - loss: 0.6911 - val_accuracy: 0.9000 - val_loss: 0.3086\n",
      "Epoch 4/15\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 158ms/step - accuracy: 0.8250 - loss: 0.3914 - val_accuracy: 0.9500 - val_loss: 0.2051\n",
      "Epoch 5/15\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 159ms/step - accuracy: 0.9000 - loss: 0.3213 - val_accuracy: 0.9500 - val_loss: 0.1775\n",
      "Epoch 6/15\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 178ms/step - accuracy: 0.8625 - loss: 0.3069 - val_accuracy: 0.9500 - val_loss: 0.1941\n",
      "Epoch 7/15\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 156ms/step - accuracy: 0.8750 - loss: 0.2993 - val_accuracy: 0.9500 - val_loss: 0.1540\n",
      "Epoch 8/15\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 160ms/step - accuracy: 0.9375 - loss: 0.2153 - val_accuracy: 0.9500 - val_loss: 0.1201\n",
      "Epoch 9/15\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 160ms/step - accuracy: 0.9375 - loss: 0.1977 - val_accuracy: 0.9500 - val_loss: 0.1161\n",
      "Epoch 10/15\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 162ms/step - accuracy: 0.9625 - loss: 0.1537 - val_accuracy: 0.9500 - val_loss: 0.1361\n",
      "Epoch 11/15\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 165ms/step - accuracy: 0.9375 - loss: 0.1607 - val_accuracy: 0.9500 - val_loss: 0.1427\n",
      "Epoch 12/15\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 162ms/step - accuracy: 0.9375 - loss: 0.1481 - val_accuracy: 0.9500 - val_loss: 0.1158\n",
      "Epoch 13/15\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 171ms/step - accuracy: 0.9750 - loss: 0.1064 - val_accuracy: 0.9500 - val_loss: 0.1012\n",
      "Epoch 14/15\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 169ms/step - accuracy: 0.9625 - loss: 0.1512 - val_accuracy: 0.9500 - val_loss: 0.1013\n",
      "Epoch 15/15\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 173ms/step - accuracy: 0.9750 - loss: 0.1083 - val_accuracy: 0.9500 - val_loss: 0.0933\n",
      "âœ… Modelo salvo com sucesso!\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ConfiguraÃ§Ãµes de Treino\n",
    "BATCH_SIZE = 16\n",
    "IMG_SHAPE = (160, 160)\n",
    "DATA_DIR = os.path.join(\"data\", \"raw\") # Pega direto da pasta onde salvamos\n",
    "\n",
    "# 1. Carregamento Inteligente (JÃ¡ divide Treino/ValidaÃ§Ã£o e Redimensiona)\n",
    "train_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "    DATA_DIR,\n",
    "    validation_split=0.2,\n",
    "    subset=\"training\",\n",
    "    seed=123,\n",
    "    image_size=IMG_SHAPE,\n",
    "    batch_size=BATCH_SIZE\n",
    ")\n",
    "\n",
    "val_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "    DATA_DIR,\n",
    "    validation_split=0.2,\n",
    "    subset=\"validation\",\n",
    "    seed=123,\n",
    "    image_size=IMG_SHAPE,\n",
    "    batch_size=BATCH_SIZE\n",
    ")\n",
    "\n",
    "class_names = train_ds.class_names\n",
    "print(f\"\\nğŸ¤– Classes encontradas: {class_names}\")\n",
    "\n",
    "# 2. Performance (Cache na memÃ³ria para ser rÃ¡pido)\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "train_ds = train_ds.cache().shuffle(1000).prefetch(buffer_size=AUTOTUNE)\n",
    "val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "\n",
    "# 3. Data Augmentation (Cria variaÃ§Ãµes das fotos para o modelo aprender melhor)\n",
    "data_augmentation = tf.keras.Sequential([\n",
    "    tf.keras.layers.RandomFlip(\"horizontal\"),\n",
    "    tf.keras.layers.RandomRotation(0.1),\n",
    "    tf.keras.layers.RandomZoom(0.1),\n",
    "])\n",
    "\n",
    "# 4. ConstruÃ§Ã£o do Modelo\n",
    "base_model = tf.keras.applications.MobileNetV2(input_shape=(160, 160, 3),\n",
    "                                               include_top=False,\n",
    "                                               weights='imagenet')\n",
    "base_model.trainable = False\n",
    "\n",
    "# CORREÃ‡ÃƒO DO BUG: O preprocessamento fica DENTRO do modelo\n",
    "preprocess_input = tf.keras.applications.mobilenet_v2.preprocess_input\n",
    "\n",
    "inputs = tf.keras.Input(shape=(160, 160, 3))\n",
    "x = data_augmentation(inputs)\n",
    "x = preprocess_input(x) # <--- AQUI A MÃGICA: Converte 0-255 para -1 a 1 automaticamente\n",
    "x = base_model(x, training=False)\n",
    "x = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
    "x = tf.keras.layers.Dropout(0.2)(x)\n",
    "outputs = tf.keras.layers.Dense(len(class_names), activation='softmax')(x)\n",
    "\n",
    "model = tf.keras.Model(inputs, outputs)\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# 5. Treinar\n",
    "history = model.fit(train_ds, validation_data=val_ds, epochs=15)\n",
    "\n",
    "# Salvar\n",
    "model.save(\"face_classifier.keras\")\n",
    "print(\"âœ… Modelo salvo com sucesso!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "73b1cb27-b011-4f9b-8c52-07311ab8d792",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¥ Iniciando reconhecimento... [Q] para sair\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import mediapipe as mp\n",
    "\n",
    "# Carrega o modelo\n",
    "model = tf.keras.models.load_model(\"face_classifier.keras\")\n",
    "class_names = [\"alex\", \"fabiana\"] # Importante: Ordem alfabÃ©tica (padrÃ£o do diretÃ³rio)\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "mp_face = mp.solutions.face_detection\n",
    "\n",
    "print(\"ğŸ¥ Iniciando reconhecimento... [Q] para sair\")\n",
    "\n",
    "with mp_face.FaceDetection(min_detection_confidence=0.6) as face:\n",
    "    while cap.isOpened():\n",
    "        ok, frame = cap.read()\n",
    "        if not ok: break\n",
    "        \n",
    "        frame = cv2.flip(frame, 1)\n",
    "        rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        results = face.process(rgb)\n",
    "        \n",
    "        if results.detections:\n",
    "            for detection in results.detections:\n",
    "                # 1. Recorte do Rosto\n",
    "                bbox = detection.location_data.relative_bounding_box\n",
    "                h, w, _ = frame.shape\n",
    "                x, y = int(bbox.xmin * w), int(bbox.ymin * h)\n",
    "                w_box, h_box = int(bbox.width * w), int(bbox.height * h)\n",
    "                \n",
    "                # Expande um pouquinho a caixa para pegar o rosto todo\n",
    "                face_crop = frame[max(0, y):min(h, y+h_box), max(0, x):min(w, x+w_box)]\n",
    "                \n",
    "                if face_crop.size > 0:\n",
    "                    # 2. Prepara imagem para o modelo\n",
    "                    img = cv2.resize(face_crop, (160, 160))\n",
    "                    img_array = np.expand_dims(img, axis=0) # Cria o lote de 1 imagem\n",
    "                    \n",
    "                    # 3. PrediÃ§Ã£o (O modelo jÃ¡ faz a normalizaÃ§Ã£o internamente!)\n",
    "                    prediction = model.predict(img_array, verbose=0)\n",
    "                    index = np.argmax(prediction)\n",
    "                    confidence = prediction[0][index]\n",
    "                    \n",
    "                    # 4. Resultado\n",
    "                    nome = class_names[index].upper()\n",
    "                    cor = (0, 255, 0) if confidence > 0.8 else (0, 0, 255)\n",
    "                    texto = f\"{nome} ({confidence:.0%})\"\n",
    "                    \n",
    "                    cv2.rectangle(frame, (x, y), (x+w_box, y+h_box), cor, 2)\n",
    "                    cv2.putText(frame, texto, (x, y-10), cv2.FONT_HERSHEY_SIMPLEX, 0.8, cor, 2)\n",
    "        \n",
    "        cv2.imshow(\"Reconhecimento Facial\", frame)\n",
    "        if cv2.waitKey(1) == ord('q'):\n",
    "            break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8faee7bb-5a0a-494c-861a-9376006d496d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (faceproj)",
   "language": "python",
   "name": "faceproj"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
